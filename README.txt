FALL 2017, SPRING 2018 CAPSTONE README
"GESTURE RECOGNITION UTILIZING INFRARED SENSORS"

---------------------
PROJECT DESCRIPTION
---------------------

THE PRIMARY GOAL OF OUR PROJECT WAS TO START DEVELOPMENT ON A SEMI-AUTONOMOUS ROBOT CONTROLLED BY GESTURES TO HELP AID PHYSICALLY DISABLED INDIVIDUALS. ONE OF 
THE MAIN PROBLEMS IN THIS SITUATION BEING THAT PHYSICALLY DISABLED INDIVIDUALS DO NOT ALL HAVE THE SAME DISABILITIES MEANING THAT A DYNAMIC ARRAY OF
GESTURES HAD TO BE ESTABLISHED FOR USE OF THE ROBOT'S FUNCTIONS BY DIFFERENT INDIVIDUALS. OUR GOAL EXPANDED FROM SOLEY BEING USED BY PHYSICALY DISABLED INDIVIDUALS
TO MILITARY, INDUSTRIAL, AND CONSUMER APPLICATIONS AS WELL AS OUR DYNAMIC GESTURE RECOGNITION SYSTEM DEVELOPED. 

TO DO DEVELOP THIS SYSTEM, WE UTILIZED TRANSFER LEARNING OF THE ALEXNET DEEP CONVOLUTIONAL NEURAL NETWORK THROUGH MATLAB'S NEURAL NETWORK TOOLBOX. WE DELETED
THE LAST THREE FULLY CONNECTED LAYERS (WHICH ARE TYPICALLY USED FOR OBJECT CLASSIFICATION) WHILE KEEPING THE FIVE CONVOLUTIONAL LAYERS INTACT AND 
REWROTE THE LAYERS TO OUR SPECIFICATIONS AND USING IMAGES OF GESTURES WE CREATED. 

WE AUTOMATED THE PROCESS OF GESTURE CREATION AND TRAINING THROUGH IMPLEMENTATION OF A GUI THROUGH MATLAB AND A FEW OTHER MATLAB FUNCTIONS WHICH WILL BE
EXPLAINED FURTHER IN A FOLLOWING SECTION OF THIS FILE. 

---------------------
HOW THE PROGRAM WORKS
---------------------

THE PROGRAM, IN ITS ENTIRETY, USES THE NUC COMPUTER ON THE ROBOT WITH THE MONO-INFRARED CAMERA (ASSUMING IT'S STILL ATTACHED TO THIS ROBOT BY THE TIME 
THIS PROJECT IS REASSIGNED), THE TOWER WITH THE EXPENSIVE GPU, AND A RASPBERRY PI. THE NUC USES THE COMPUTER.M FILE WHILE THE TOWER USES THE GUI.M FILE.

THE NUC COMPUTER CAPTURES IMAGES ON THE INFRARED CAMERA AT 640 X 480 RESOLUTION. THE IMAGE IS COMPRESSED TO A 227 X 227 (THE SIZE REQUIRED FOR THE NEURAL NETWORK) 
ARRAY, RESIZED TO A LINEAR VECTOR (IT DOESN'T WORK WITH THE TCP/IP MATLAB FUNCTION OTHERWISE), AND TRANSMITTED WIRELESSLY TO THE TOWER VIA THE AFOREMENTIONED TCP/IP
MATLAB COMMUNCATION FUNCTION. 

THE TOWER WILL NOT READ THE TRANSMITTED IMAGE UNTIL THE "START PROGRAM" BUTTON ON THE GUI IS PRESSED. ONCE THIS HAPPENS, THE TOWER READS THE TRANSMITTED IMAGE AND RESHAPES
IT TO A 227 X 227 IMAGE FROM ITS LINEAR VECTOR FORM. THE GUI WILL PROMPT THE USER "NEW GESTURE (Y/N)." IF "N," THE GUI WILL LOAD THE EXISTING NEURAL NETWORK .ASV FILE FROM
THE USER'S CURRENT DIRECTORY (NOTE THIS MAY TAKE A FEW MINUTES). IF "Y," THE GUI WILL PROMPT "WHAT IS THE NAME OF NEW GESTURE?" THE USER MUST THEN ENTER THE NAME 
OF THEIR GESTURE WHICH WILL ADD AN IDENTICALLY NAMED FOLDER TO THE "DATA" FOLDER (THE DATA FOLDER WILL BE CREATED IN THE USER'S CURRENT DIRECTORY IF IT DOES NOT ALREADY 
EXIST). THE USER WILL THEN BE ASKED TO ENTER A DELAY TIME. THIS WILL GIVE THE USER TIME TO PREPARE TO CREATE THE GESTURE. AFTER THE DELAY PERIOD HAS EXPIRED, THE TOWER 
WILL READ IN 100 IMAGES TRANSMITTED FROM THE NUC'S INFRARED CAMERA. EACH IMAGE WILL BE BINARIZED AND FILTERED SUCH THAT ANOMOULOUS HEAT SOURCES THAT ARE SMALLER IN WHITE
PIXEL COUNT ARE FILTERED OUT OF THE IMAGE. THIS IS TO PREVENT BACKGROUND DATA FROM INTERFERING WITH THE NEURAL NETWORK. ONCE ALL 100 IMAGES (NOTE THAT NEURAL NETWORK 
PERFORMANCE CAN BE INCREASED WITH MORE IMAGES AT THE EXPENSE OF TRAINING TIME) ARE CAPTURED, THE USER WILL BE  ASKED IF THEY WOULD LIKE TO CREATE ANOTHER GESTURE. IF "Y," 
THE PROCESS WILL REPEAT ITSELF. OTHERWISE, THE PROGRAM WILL BEGIN TRAINING ALL GESTURES CONTAINED WITHIN THE "DATA" FOLDER FOR THE SET NUMBER OF ITERATIONS SPECIFIED IN THE
GUI.M MATLAB FILE. ONCE THE DATA IS TRAINED IT WILL BE EXPORTED TO A .ASV FILE FOR LATER USE WHEN NO ADDITIONAL GESTURES ARE ADDED. 

WHEN THE PROGRAM HAS FINISHED TRAINING, THE USER WILL BE ABLE TO STAND IN FRONT OF THE CAMERA TO SEE THEIR BINARIZED IMAGE ON THE RIGHT SIDE OF THE PANEL AND A "SCORE"
CHART ON THE LEFT SIDE OF THE PANEL. THE "SCORE" CHART DISPLAYS A BAR GRAPH OF THE NEURAL NETWORK'S TOP 5 PREDICTIONS ON WHAT GESTURE IS BEING PERFORMED BY THE USER AND
THEIR RESPECTIVE ACCURACIES. IF THE GESTURE IS NOT LABELED "NOISE" (A TRAINED GESTURE USED TO PREVENT BACKGROUND NOISE FROM BEING MISIDENTIFIED AS A GESTURE BEING PERFORMED
AND THE GESTURE'S SCORE PROBABILITY EXCEEDS A DEFINED THRESHOLD (OURS WAS SET AT 80%) THE GESTURE WILL BEGIN TO BE IDENTIFIED. THIS IS SHOWN BY THE "CONFIRMING" PERCENTAGE
VALUE DISPLAYED ABOVE THE BINARIZED DISPLAY. THIS CAN BE INCREASED/DECREASED WITHIN THE MATLAB FILE, BUT IT TAKES ABOUT 4-5 SECONDS FOR A GESTURE TO BE IDENTIFIED AND WAS
IMPLEMENTED TO PREVENT MISIDENTIFIED GESTURES. AFTER THIS PERIOD, THE GESTURE LABEL WILL BE DISPLAYED TO THE USER. IF THE DISPLAY CONDITIONS ARE NOT MET, THE IMAGE LABEL
WILL READ "GESTURE NOT RECOGNIZED." 

THE PROGRAM CAN BE PAUSED SUCH THAT DATA IS NOT RECEIVED BY THE TOWER BY HITTING THE "END" BUTTON ON THE GUI AND CAN BE RESUMED BY HITTING "RESUME." FURTHERMORE, IF THE 
"GESTURE CREATION" BUTTON IS PRESSED WHILE THE PROGRAM IS RUNNING, THE USER WILL BE ABLE TO USE A DROP DOWN MENU TO SEE ALL GESTURES CURRENTLY RECOGNIZED BY THE NEURAL 
NETWORK. 

THE GESTURE2COMMAND.M FILE CONTAINS ALL GESTURE NAMES THAT WILL RESULT IN A COMMAND. THIS INFORMATION IS WRITTEN INTO THE COMMAND ARRAY AND IS TRANSMITTED TO THE NUC ONBOARD
COMPUTER. THIS COMMAND ARRAY CONTAINS INFORMATION TO PERFORM DESIGNATED FUNCTIONS. THE FIRST CELL OF THE ARRAY CURRENTLY HAS NO FUNCTION AS ITS ORIGINAL FUNCTION DID NOT 
WORK AS INTENDED. CELLS 2 AND 3 ARE FOR LARGE CAMERA SWIVEL MOVEMENTS WHEN THE USER NEARS THE EDGE OF THE FRAME WHILE CELLS 4 AND 5 ARE REFINED CAMERA MOVEMENTS USED FOR 
CENTERING THE USER (NOTE THIS REQUIRES PUTTY SOFTWARE, A STEPPER MOTOR, AND A STEPPER FUNCTION. THE REST OF THE CELLS ARE USED FOR GESTURES DEFINED IN THE GESTURE2COMMAND 
FUNCTION AND CAN BE USED TO MOVE THE ROBOT OR PERFORM OTHER FUNCTIONS ON CONNECTED EQUIPMENT. 

---------------------
GENERAL NOTES
---------------------

1) THE NUC IS CURRENTLY SET AS THE SERVER AND THE TOWER IS THE CLIENT IN THE TCP/IP COMMUNCATION FUNCTION. THE SERVER MUST ALWAYS BE STARTED BEFORE THE CLIENT OR THE PROGRAM
   WILL CRASH.

2) ENSURE THAT THE IP IN THE TCP/IP COMMUNCATION FUNCTIONS ARE CORRECT. NO STATIC IP IS SET SO THESE WILL SOMETIMES CHANGE. THE SERVER SHOULD CONTAIN THE CLIENT'S IP IN THE FUNCTION 
   AND VICE VERSA. THE PROGRAM WILL CRASH OTHERWISE.

3) ENSURE THE CAMERA IS ALWAYS ON OR THE PROGRAM WILL CRASH. 

4) ADD A SMALL PAUSE FUNCTION (0.1 SECONDS OR POSSIBLY LESS) BEFORE READING HANDLE DATA IN THE GUI. THE DATA MAY NOT READ IN OTHERWISE.

5) THE NEURAL NETWORK SETTINGS CAN BE CHANGED IN THE CODE TO INCREASE ACCURACY. THE MORE ITERATIONS THAT ARE PERFORMED, THE MORE ACCURACY YOU WILL HAVE BUT THE PROGRAM 
   WILL TAKE LONGER TO TRAIN, IMPORT, AND EXPORT THE NEURAL NETWORK DATA. 

6) THE INFRARED CAMERA DID NOT WORK WITH THE UPS AS THE IMAGE WOULD BECOME DISTORTED AND WOULD BE READ AS A NULL VECTOR BY THE PROGRAM FUNCTIONS THUS CRASHING IT. WE
   MEASURED THE POWER OUTLETS WITH AN OSCILLOSCOPE AND THE OUTPUT WAS NORMAL. PERHAPS TRY SOME SORT OF RF SHIELDING AROUND THE CAMERA?

7) THERE IS A DEBUG MODE PROGRAMMED INTO THE GUI.M FILE THAT WILL SKIP THE REGULAR RUN MODE OF THE GUI. TO DISABLE/ENABLE THIS, SIMPLY SET THE DEBUG VARIABLE IN THE CODE ACCORDINGLY (1 OR 0)

-----------------------------
SUGGESTIONS FOR IMPROVEMENT
-----------------------------

1) WE RECCOMEND INTEGRATING THE RGBD (INTEL REALSENSE) CAMERA WE PURCHASED FOR OUR PROJECT. WE RAN OUT OF TIME TO INTEGRATE IT BUT IT WILL ADD A FULL DIMENSION TO THE PROJECT 
   AND WILL ALLOW YOUR TEAM TO BETTER FILTER DATA AND PERFORM MORE COMPLEX GESTURES. USE THE METADATA FROM THE CAMERA TO ACCOMPLISH THIS. 
   
		***THE SDK FOR THE CAMERA IS ALREADY LOADED INTO THE NEW (ALL BLACK) NUC; USERNAME: MOTAI_NUC, PASSWORD: VCU2018ece***
		   EXAMPLES FOR THE R200 CAN BE FOUND IN A FOLDER ON THE DESKTOP, OPEN "SDK_BROWSER" TO VIEW RUN EXAMPLES AND OPEN SOURCE CODE FOR EACH


2) THERE ARE A FEW THINGS THAT CAN BE USED TO IMPROVE THE NEURAL NETWORK THAT WE DID NOT HAVE TIME TO DO. IT'S RECCOMENDED THAT YOU EXTRACT FEATURES FROM AN IMAGE
   AND FEED THIS DATA INTO THE NEURAL NETWORK WITH THE IMAGE. YOU CAN USE THE SIFT_FEATURE OR DETECTSURFFEATURES FUNCTIONS TO ACCOMPLISH THIS (THE FUNCTIONS ARE IN THE SAME FOLDER AS 
   THE GUI.M FILE BUT ARE ALSO PUBLICLY AVAILABLE TO DOWNLAOD. 

3) FEEDING A SKELETAL BODY DIAGRAM INSTEAD OF JUST A BINARIZED INDVIDUAL WOULD HELP INCREASE ACCURACY AS THE NEURAL NETWORK CAN MISINTERPERET GESTURES OF DIFFERENT BODY TYPES 
   (TALL/SHORT OR LARGE/SMALL BODY FRAMES). 

4) THERE ARE LIKELY SEVERAL WAYS OUR PROJECT CAN BE OPTIMIZED TO GET A FASTER FPS SO YOU MAY WANT TO LOOK AT HOW THIS MAY BE DONE. 

5) LOOK INTO IMPLEMENTING A MORE DYNAMIC SYSTEM SUCH THAT A USER CAN CREATE A GESTURE AND ASSIGN IT TO A DROP DOWN LIST OF AVAILABLE FUNCTIONS. 